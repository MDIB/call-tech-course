- Nonlinear transformation
  - All models are linear in `w` as in `w^t * x`, that said we can transform x through any function `g` it will still be liner in `w`

- Error and noise.
  - First lets modify the learning diagram we already have to incorporate the notion of learning and the notion of noise.
  

  
   ---------------------------------        -------------------
   |   Unknown target distribution |        |  Probability    |
   |   + noise                     |        |  Distribution   | 
   ---------------------------------        -------------------   
                  |                     
		  |                             |     |
		  |                             |     |
		  V                             |     |
	-----------------------                 |     |
	| Training examples   | <----------------     -------------------------
	-----------------------              -------------------              |
	          |                          | Error measure   |              v
		  |                          -----|-------------------->  g(x) ~ f(x)
		  |                   ____________V___                 ______________________
		  ------------------> |    Learning  | --------------> |  Final Hypotesis   |
		  ------------------> |   Algorithm  |                 ----------------------
		  |                   ----------------
		  |
		  |
        ------------------
	| Hypotesis Set  |
	------------------


- Error measure `E(h,f)` i.e `h~f`,
  - Pointwise definition `e(h(x),f(x))` => e.g squared error (linear regression) -> (h(x)-f(x))^2
  - Binary error e(h(x),f(x))= [[h(x) != f(x)]] (where [[]] means if true 1 else 0)
  - Overall error (from pointwise view) E(h,f) = average of pointwise errors 
  - IN-sample error : Ein(h) = (1/N) * Sum(n=1,N,e(h(xn),f(xn))) (frequency of the error in the sample)
  - Out-of-sample error Eout(h) = Ex[e(h(x),f(x))], where Ex is the expected error (because of the probability dist over X) 
  - Use Probability distribution (Hoefiding) to generate X and test the error measure


- How to choose error measure
  - Example - fingerprint verification (false accept or false reject) 

              f
      | +1         |   -1 
 -----------------------------------------------------------------------------
   +1 |no error    | false accept
 h    |            |
   -1 |false reject| no error

 Market that want to give customers offers when they use their fringtip, in this case false reject is costly,
 a customer that has the offter would have an horrible experience, for business is not that important to give
 it away discounts in order to not lose customer. So, in this situation we would penalize our algorithm by:
            f
    |  +1      -1 
 ---------------------------------------------------------------------------
  +1| 0        1 
 h  |
  -1| 10       0

 As we can see the algorithm would be penalized by ten times if it false rejects someone

            f
    |  +1      -1 
 ---------------------------------------------------------------------------
  +1| 0        10000
 h  |
  -1| 1       0

 In this case we penalize the algorithm by 10000 times when it false accepts someone.


 Another use-case, the Pentagon wants to use your learning model of fingerprints on their door, in this case
 false accepts is much worst than false accept so we would do something like :

 - Use the customer's situation to adequate the errors.
 
- Noisy targets
 - The "target function" it's not always a function i.e two "identical" customer -> two different behaviours
 - Instead of y = f(x) we use target disribution `P(y | x)` 
 - (x,y) is now generated by the join distribution. `P(x)*P(y|x)` i.e `y` is not determinist anymore once `P(x)` is generated (hoeffiding)


- Difference between `P(x)` and `P(y|x)` 
  :wq




Questions 
  Didn't understood that ->  Eout(h) = Ex[e(h(x),f(x))], where Ex is the expected error (because of the probability dist over X) 

